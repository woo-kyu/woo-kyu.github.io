---
layout: single
title: ResNet Paper Review
toc_label: ResNet Paper Review
categories: 'Deep_Learning'
tags: [Deep Learning, CNN]
author_profile: false
search: true
use_tex: true
---

> ResNet Paper Review

[Original Paper](https://arxiv.org/pdf/1512.03385)

# 개요

<hr>
<hr>

- 일반적으로 CNN 모델은 층이 더 많아(깊어)질 수록 더 복잡한 특징을 학습 할 수 있다.
- 복잡한 패턴을 이해하기 위해서는 층이 깊어질 수 밖에 없다.
- 그러나,  네트워크의 깊이가 깊을수록 Gradient vanishing(기울기 소실)과 Gradient explosion(기울기 폭발) 문제 가진다.
- 이러한 문제는 모델이 학습을 잘 하지 못하는 결과를 초래한다.
- 이 문제를 해결하기 위해 **Residual[^1] learning(잔차 학습)** 기법을 사용했다.

<br>

- 그 결과 깊은 깊이의 신경망 학습에서, Plain Network 에 비해 Residual Network 가 더 낮은 에러율을 보여준다.
  - 심지어, 추가적인 계산 복잡도를 요구하지 않는다!

<img width="1000" alt="Figure" src="https://github.com/user-attachments/assets/0e2d0e7c-d1ee-4858-85c3-0404d02a80e0">{: .align-center}

<br>


# 문제 제시

<hr>
<hr>

- 기존 Convolution Neural Network(CNN)은, 레이어가 깊어질수록 더 많은 특징을 추출, 학습할 수 있다.
  - = 네트워크의 깊이가 깊어질 수록 더 많은 특징 레벨을 추출할 수 있다.

<br>

- 그러나, 단순히 레이어만 깊게 쌓는 것은 여러 문제가 발생한다.
- 대표적으로, Gradient Vanishing, Gradient Explosion 문제 등이 발생한다.
- 이러한 문제는 VGG 네트워크에서 제안한: normalized initialization, intermediate normalization layers 기법을 통해 일부 해결되었다.
  - To start converging for stochastic gradient descent(SGD) with back-propagation
    - SGD 와 역전파 기법을 통해 네트워크가 성공적으로 converging(수렴, 학습) 할 수 있도록 한다.
  - 중간 정규화 레이어: Batch Normalization(BN) 과 같은 기법을 사용하여 네트워크의 중간에서 데이터를 정규화
  - Normalized initialization(정규화된 초기화): 신경망의 가중치를 적절히 초기화하여, 네트워크의 학습 초기에 기울기 소실 문제를 줄이는 방법

<br>

- 이러한 기법을 사용함에도 불구하고, 더 깊은 레이어를 사용할 때 수렴이 가능 하지만, degradation problem has been exposed. (성능 저하 문제가 발생한다.)
- 네트워크 깊이가 증가함에 따라 정확도가 saturated(포화 상태)에 이르렀다가(더이상 증가하지 않는 시점), 감소하게 된다.
- 이러한 성능 저하 문제는 Over-fitting 문제가 아니며,
- 적절한 깊이를 가진 모델에 더 많은 레이어를 추가할 때, leads to higher training error.

<img width="600" alt="ut" src="https://github.com/user-attachments/assets/8ceadc64-77f6-4d33-9041-f637b335da73">{: .align-center}

- Figure 1. Plain network 에서, 레이어가 깊어질 수록 오류율이 커지는 모습을 볼 수 있다.
- Identity mapping(항등 함수) 기법이 추가된 더 깊은 네트워크는 레이어가 깊어질수록 성능 저하 문제를 해결하기 위해 잔차 학습을 수행하며, 항등 함수를 통해 입력을 그대로 전달하지만, 여전히 최적화 과정에서 학습 오류가 발생할 가능성은 존재한다.

<br>

# 개선 방법 제안

<hr>
<hr>

## Core: Residual Learning

> Overview

이 논문에서는 이러한 문제의 해결 방법으로 Residual Learning(잔차 학습) 기법을 제시한다.

- 명시적으로 residual mapping 을 정의하여 학습하는 것.

<img width="600" alt="youtube_나동빈 출처" src="https://github.com/user-attachments/assets/13fc0a28-33d7-4000-8f5d-8f5f4a8bbd0a">{: .align-center}

<br>

- $H(x)$ 는 의도한, optimal function 으로, 신경망이 여러 개의 비선형 레이어들을 통해 점진적으로 근사해야 하는 목표 함수이다.
  - 일반적인 Neural network 의 역할은 여러개의 non-linear 한 layer 를 이용해서 점진적으로, 복잡한 함수를 근사(학습)하는 것이 목표이다.
- 이는 residual function 를 근사하는 것과 동일하다.
- 즉, $H(x)$를 학습하는 것 보다 **$F(x) = H(x)-x$** 를 학습하는 것인데,
- 이는 네트워크가 전체 목표 함수를 학습하는 것이 아니라, 입력과 출력 사이의 차이(잔차) 만 학습하는 것이다.
- 이 구조는 결과적으로, residual function $F(x)$ 를 학습하여 최종 출력으로 $F(x)+x$ 형태가 된다.
- 이는 $H(x)$를 학습하는 것 보다 더 효율적이다.

<br>

### Additional Explain

- $H(x)$: 신경망이 학습해야 할 목표함수. 즉, **$x$** 를 통해 얻고자 하는 최종 출력 값
- **$x$**: **입력 값 $x$** 와 동일하지만, 이를 **목표 함수 $H(x)$** 에서 뺀다.
  - I.e., **$H(x)-x$** 는 **목표 함수 $H(x)$** 와 입력 $x$ 간의 차이를 의미한다.
  - 이를 Residual (잔차) 라고 이야기하며, 네트워크가 학습해야 할 부분이다.
- 이 표현의 의미는, **잔차 함수 $F(x) = H(x) - x$** 를 학습함으로써, 네트워크는 **$x$** 로 부터 어떤 변화(잔차)를 학습하게 된다는 것이다.
  - 네트워크는 입력 **$x$** 와 **목표 출력 $H(x)$** 사이의 잔차를 학습하는 것
- 최종적으로, 네트워크는 **잔차 $F(x)$** 를 학습한 후, **$x$** 에 이를 더해 $F(x)+x$ 형태로 최종 출력을 얻는다.
  - 여기에서 **$F(x)$** 는 **$H(x)-x$**로 부터 계산된 것이므로, 결과적으로 **$H(x)$** 를 얻는것과 같다.

<br>

## Identity Mapping By Shortcuts

<img width="1000" alt="youtube_나동빈 출처" src="https://github.com/user-attachments/assets/a08f809c-74d7-4ee7-a420-23b0ba89a135">{: .align-center}

### Residual Block

- 잔차 학습을 몇 개의 층마다 묶어 적용한다.

<img width="600" alt="ut" src="https://github.com/user-attachments/assets/4b273778-9002-49a5-b78f-a6180caf00d9">{: .align-center}

- 이러한 묶음 덩어리를 Block 이라고 칭한다.
- Block 의 구조는 다음과 같이 정의된다.
  - $y=F(x,$ { $W_{i}$ } $)+x$
    - Block 의 x, y 는 각각 input, output 벡터 이다.
    - $F(x,$ { $W_{i}$ } $)$는 학습 할 residual mapping 을 의미한다.
      - 이 함수는 여러개의 레이어와 매개변수 (Weight $W_{i}$) 로 이루어져 있다.
- E.g., Fig 2. 의 2개의 레이어에서 $F=W_{2}\sigma (W_{1}x$ 로 표현될 수 있다.
  - **$\sigma$** 는 non-linear activation function (비선형 활성화 함수, ReLU)

<br>

### Shortcut Connection (단축 연결)

- Residual function $F(x,$ { $W_{i}$ } $)$ 와 입력 $x$ 를 엘리먼트 단위로 더해주는 역할 수행
- 이를 통해 neural network 가 residual function 만 학습하고, 입력은 그대로 다음층으로 전달한다.
- 추가적인 파라메터가 사용되지 않기 때문에 계산 복잡도가 증가하지 않는다.

<br>


### Identity Mapping

> 입력값을 그대로 출력하는 함수, 항등함수 $f(x) = x$

> Deep Learning; 레이어를 거치면서 입력이 변형되지 않고 그대로 출력되는 경우.
>
> 즉, 네트워크의 특정 층이나 shortcut connection 을 통해 입력 데이터를 그대로 다음 층으로 전달하는 것

> **Residual Learning**
>
>잔차 학습에서는 네트워크의 각 층이 복잡한 합수를 직접 학습하는 대신, 잔차(변화의 차이)를 학습하도록 설계.

- 이때 Identity mapping 는 중요한 역할을 수행하는데, optimal function 이 identity mapping 일 경우, 네트워크는 입력을 그대로 출력하게 만들어야 한다.
- 항등 함수를 학습하는 것이 목표라면, 네트워크가 여러 비선형 층을 통해 복잡한 변형을 학습하는 것 보다, residual 를 0으로 만드는 것이 더 쉽기 때문.


<br>

#### Identity mapping 의 역할

##### Shortcut connection

- 단축 연결
- Residual Network 에서 단축 연결을 사용해 입력을 다음 레이어로 그대로 전달하는 경우, 이 연결이 항등 함수를 수행하게 된다.
- 입력값이 변형되지 않고 다음 레이어로 전달되므로, 네트워크가 쉽게 최적화 할 수 있게 만든다.

<br>

##### 최적화 단순화

- 네트워크가 항등 함수를 학습할 때, 여러 층을 통과하며 복잡한 변환을 학습하는 것 보다, residual 를 0으로 만들어 항등 함수에 가깝게 만드는 것이 더 쉽다.

<br>

### Dimension Matching

- $x$ 와 $F(x,$ { $W_{i} $ } $ )$는 차원이 동일해야 하지만, 차원이 다를 경우(예를들어, 입력과 출력의 채널 수가 다른 경우) 다음 기법을 따른다.
- Linear Projection(선형 투영) $W_{s}$ 를 사용하여 차원을 일치시킨다.
- 이를 통해 잔차 함수와 입력 간 덧셈이 가능하다.
  - $y=F(x, $ { $ W_{i} $ } $)+W_{s}x$

<br>
  
- Identity shortcuts 는 입력과 출력이 같은 차원일 때 직접 사용되나, 차원이 증가할 때 다음 옵션을 고려할 수 있다.
- 아래 옵션을 적절히 사용해야 한다.
- 특정 아키텍처 모델에 따라 장 단점이 존재.

#### Option A

- Identity mapping 을 사용하지만, 입력과 출력의 차원을 맞추기 위해 Zero padding 을 사용한다.
- 추가적인 파라메터 또는 모델 복잡도를 증가시키지 않음

<br>

#### Option B

- 1x1 Convolution 을 사용한 projection shortcut 을 적용합니다.
- 학습을 위한 추가적인 가중치가 필요하다.

<br>

#### Stride of 2

- 차원이 다른 두 레이어 사이에서 shortcut 을 적용할 때는 stride of 2 방법으로 shortcut 을 적용.
- feature map 의 크기가 레이어 사이에서 축소될 때 주로 사용되는 방식.

<br>

##### using example

<img width="400" alt="u" src="https://github.com/user-attachments/assets/055bf658-8453-46b2-bae4-d170f90ad782">{: .align-center}

<img width="400" alt="" src="https://github.com/user-attachments/assets/aef07319-7bc5-4fa4-91e0-6939a6b35ce1">{: .align-center}

- ResNet- 34 A, B, C 는 각각의 기법을 달리한 결과이다.
- (A) zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameter-free (the same as Table 2 and Fig. 4 right); 
- (B) projection shortcuts are used for increasing dimensions, and other shortcuts are identity
- (C) all shortcuts are projections.

<img width="600" alt="ut" src="https://github.com/user-attachments/assets/eee48c01-6cd1-4428-90c8-efc3bcaad684">{: .align-center}

<br>

#### Flexible

- Residual Function $F$ 의 형태는 유연하다.
- (residual block 마다) 두 개 이상의 레이어를 포함할 수 있지만, 한 개의 단일 레이어에서 사용 할 경우 선형 레이어와 유사하게 동작한다.
  - 이는 곧 have ont observed advantages. (성능 증가가 없다.)
  - $F(x,$ { $W_{i} $ } $)$ 가 여러 weight 의 중첩한 정보를 가질 때 성능 향상이 존재한다.
- Fully connected layer 뿐만 아니라, convolution layer 에서도 적용할 수 있다.

<br>

## Network Architecture

<img width="600" alt="" src="https://github.com/user-attachments/assets/66f2af15-23e0-42f6-bf21-974d0a650922">{: .align-center}

<img width="800" alt="" src="https://github.com/user-attachments/assets/0e774ff4-0fab-4750-9c1b-9cdcab5471b7">{: .align-center}

- Each convolution layer 를 거칠 때 마다 Batch Normalization(BN) 사용
- Learning rate 를 점점 줄여가는 테크닉을 사용.

<br>

## Figure

- 기존 Plain network 에서, 네트워크의 깊이를 증대시킬 때 발생하는 에러율 증가는 vanishing gradients 의 이유로 예측되었으나, 그렇지 않았다.
- 학습 과정에서 forward pass 또는 backward pass 에서 어떠한 signal 값이 점진적으로 사라지는 현상을 발견하지 못하였다.
- 오히려, exponentially low convergence rate. 즉, 이러한 수렴률이 기하급수적으로 낮아지는 것이 문제인 것으로 예측한다.
  - convergence rate: 최적화 기법 중 일부로, 수렴을 위해 필요한 epoch 이나 수렴 난이도를 이야기하는 척도
- 너무 불필요하게 깊은 깊이는 당연하게도, Over-fitting 을 야기하기 때문에 주의해야 한다.

<br>

<img width="600" alt="" src="https://github.com/user-attachments/assets/c1909ecf-3432-4c22-b8f7-6ccc95457b07">{: .align-center}

<img width="400" alt="" src="https://github.com/user-attachments/assets/636c0dde-507c-40ea-8f00-4ab21e2faebe">{: .align-center}



<br>

## Footnote

### 1: Residual(잔차)

> Regression(회귀 분석)에서 관측값에서 회귀식에 의한 추정량을 뺀 값

- 잔차 = 관측값 - 예측값


- 오차와의 차이
  - 오차는 모집단의 회귀식에 대한 편차값인데 반해, 잔차는 표본 집단의 회귀식에 대한 편차값.
  - I.e., 오차는 관측값을 톡해 예측한 가정이 실제와 얼마나 부합하는 지의 정도.
  - 잔차는 예측한 가정이 관측값을 얼마나 잘 반영하고 있는지에 대한 의미.

<br>

Reference[^2]


Residual[^1]: 잔차 학습
Reference[^2]: Deep Residual Learning for Image Recognition Paper / Youtube 동빈 나, ResNet: Deep Residual Learning for Image Recognition 외 다수 영상, 블로그 