---
layout: single
title: Train End to End CNN Model
toc_label: Train End to End CNN Model
categories: Computer-Vision
tags: [Computer Vision, CNN]
author_profile: false
search: true
use_tex: true
---

# 알고리즘 개요

<hr>
<hr>

- 해당 알고리즘은 [End2End_Learning_for_Self_Driving_Cars_Paper_Review]({{site.url}}/deep-learning/end2end-learning-for-self-driving-cars-paper-review) 를 참고하여 개발하였습니다.
- ChatGPT4 의 도움 일부를 참고하여 코드를 제작하였습니다.

<br>

## 개발 목적

- 이미지를 분류함에 있어 End to End 알고리즘의 가능성을 알아보기 위해.
- 진행중인 프로젝트: 실내 환경에서의 보행 보조 장치 개발.

<br>

# Architecture

<hr>
<hr>

## Model

> ResNet50

- ResNet은 분류 작업에 특화된 CNN 모델입니다.
- Skip Connection(잔차 연결)을 통해서 네트워크의 깊이를 크게 확장하면서도 gradient vanishing 문제를 해결한 것이 특징이다.
  - 일반적으로, 신경망은 일정 깊이를 넘으면 gradient vanishing 문제와 gradient explosion 문제가 발생할 확률이 높다.
    - 이 문제에 대해서는 [Optima (Local minima problem)]({{site.url}}/machine-learning/optima) 를 참조.

- ResNet 에 대한 더 자세한 내용은 다음을 참조.
  - [ResNet Paper Review]({{site.url}}/deep-learning/resnet)

<br>

### Resnet 특징

#### Residual Learning(잔차학습)

- $H(x)$ 는 의도한, optimal function 으로, 신경망이 여러 개의 비선형 레이어들을 통해 점진적으로 근사해야 하는 목표 함수이다.
  - 일반적인 Neural network 의 역할은 여러개의 non-linear 한 layer 를 이용해서 점진적으로, 복잡한 함수를 근사(학습)하는 것이 목표이다.
- 이는 residual function 를 근사하는 것과 동일하다.
- 즉, $H(x)$를 학습하는 것 보다 **$F(x) = H(x)-x$** 를 학습하는 것인데,
- 이는 네트워크가 전체 목표 함수를 학습하는 것이 아니라, 입력과 출력 사이의 차이(잔차) 만 학습하는 것이다.
- 이 구조는 결과적으로, residual function $F(x)$ 를 학습하여 최종 출력으로 $F(x)+x$ 형태가 된다.
- 이는 $H(x)$를 학습하는 것 보다 더 효율적이다.

<br>

## Codes

[Colab](https://colab.research.google.com/drive/1e6W8wQWpPIxr698vHahAafLGR7juJPFH?usp=sharing)


<br>

## Results







